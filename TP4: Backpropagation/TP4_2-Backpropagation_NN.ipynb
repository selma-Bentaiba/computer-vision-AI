{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LdIDglk1FFcG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnUoI0m6GyjC"
   },
   "source": [
    "First let's define a neural network.  We'll just choose the weights and biases randomly for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WVM4Tc_jGI0Q"
   },
   "outputs": [],
   "source": [
    "# Set seed so we always get the same random numbers\n",
    "np.random.seed(0)\n",
    "\n",
    "# Number of hidden layers\n",
    "K = 5\n",
    "# Number of neurons per layer\n",
    "D = 6\n",
    "# Input layer\n",
    "D_i = 1\n",
    "# Output layer\n",
    "D_o = 1\n",
    "\n",
    "# Make empty lists\n",
    "all_weights = [None] * (K+1)\n",
    "all_biases = [None] * (K+1)\n",
    "\n",
    "# Create input and output layers\n",
    "all_weights[0] = np.random.normal(size=(D, D_i))\n",
    "all_weights[-1] = np.random.normal(size=(D_o, D))\n",
    "all_biases[0] = np.random.normal(size =(D,1))\n",
    "all_biases[-1]= np.random.normal(size =(D_o,1))\n",
    "\n",
    "# Create intermediate layers\n",
    "for layer in range(1,K):\n",
    "  all_weights[layer] = np.random.normal(size=(D, D))\n",
    "  all_biases[layer] = np.random.normal(size=(D, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jZh-7bPXIDq4"
   },
   "outputs": [],
   "source": [
    "# TODO Define the Rectified Linear Unit (ReLU) function\n",
    "def ReLU(preactivation):\n",
    "  return np.maximum(0, preactivation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5irtyxnLJSGX"
   },
   "source": [
    "Now let's run our random network.  The weight matrices $\\boldsymbol\\Omega_{0\\ldots K}$ are the entries of the list \"all_weights\" and the biases $\\boldsymbol\\beta_{0\\ldots K}$ are the entries of the list \"all_biases\"\n",
    "\n",
    "We know that we will need the preactivations $\\mathbf{f}_{0\\ldots K}$ and the activations $\\mathbf{h}_{1\\ldots K}$ for the forward pass of backpropagation, so we'll store and return these as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LgquJUJvJPaN"
   },
   "outputs": [],
   "source": [
    "# TODO Define the forward pass, return the final output and all intermediate outputs\n",
    "def compute_network_output(net_input, all_weights, all_biases):\n",
    "  all_f = []  # pre-activations (before ReLU)\n",
    "  all_h = [net_input]  # activations (after ReLU); first is input\n",
    "\n",
    "  current_input = net_input\n",
    "  for i in range(len(all_weights)):\n",
    "    f = all_weights[i] @ current_input + all_biases[i]\n",
    "    all_f.append(f)\n",
    "    if i < len(all_weights) - 1:\n",
    "      current_input = ReLU(f)\n",
    "    else:\n",
    "      current_input = f  # No activation function at final layer\n",
    "    all_h.append(current_input)\n",
    "\n",
    "  net_output = current_input\n",
    "  return net_output, all_f, all_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IN6w5m2ZOhnB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True output = 1.907, Your answer = 1.907\n"
     ]
    }
   ],
   "source": [
    "# Define input\n",
    "net_input = np.ones((D_i,1)) * 1.2\n",
    "# Compute network output\n",
    "net_output, all_f, all_h = compute_network_output(net_input,all_weights, all_biases)\n",
    "print(\"True output = %3.3f, Your answer = %3.3f\"%(1.907, net_output[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxVTKp3IcoBF"
   },
   "source": [
    "Now let's define a loss function.  We'll just use the least squares loss function. We'll also write a function to compute dloss_doutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6XqWSYWJdhQR"
   },
   "outputs": [],
   "source": [
    "# TODO Define the least squares loss and its derivative\n",
    "def least_squares_loss(net_output, y):\n",
    "  return 0.5 * np.sum((net_output - y)**2)\n",
    "\n",
    "def d_loss_d_output(net_output, y):\n",
    "  return net_output - y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "njF2DUQmfttR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 20.000 Loss = 163.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6460/3361061389.py:3: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"y = %3.3f Loss = %3.3f\"%(y, loss))\n"
     ]
    }
   ],
   "source": [
    "y = np.ones((D_o,1)) * 20.0\n",
    "loss = least_squares_loss(net_output, y)\n",
    "print(\"y = %3.3f Loss = %3.3f\"%(y, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98WmyqFYWA-0"
   },
   "source": [
    "Now let's compute the derivatives of the network.  We already computed the forward pass.  Let's compute the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LJng7WpRPLMz"
   },
   "outputs": [],
   "source": [
    "# TODO Main backward pass routine, it returns the derivatives of the weights and biases\n",
    "def backward_pass(all_weights, all_biases, all_f, all_h, y):\n",
    "  K = len(all_weights) - 1  # Number of layers excluding input\n",
    "\n",
    "  all_dl_dweights = [None] * (K+1)\n",
    "  all_dl_dbiases = [None] * (K+1)\n",
    "\n",
    "  # Initialize delta as derivative of loss wrt output\n",
    "  delta = d_loss_d_output(all_h[-1], y)\n",
    "\n",
    "  for layer in reversed(range(K+1)):  # Start from last layer and go backward\n",
    "    all_dl_dweights[layer] = delta @ all_h[layer].T\n",
    "    all_dl_dbiases[layer] = delta\n",
    "\n",
    "    if layer > 0:\n",
    "      df_dh = (all_f[layer-1] > 0).astype(float)  # ReLU derivative\n",
    "      delta = (all_weights[layer].T @ delta) * df_dh\n",
    "\n",
    "  return all_dl_dweights, all_dl_dbiases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9A9MHc4sQvbp"
   },
   "outputs": [],
   "source": [
    "all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PK-UtE3hreAK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Bias 0, derivatives from backprop:\n",
      "[[ -2.243]\n",
      " [  2.474]\n",
      " [  3.406]\n",
      " [ -1.942]\n",
      " [-12.468]\n",
      " [  0.   ]]\n",
      "Bias 0, derivatives from finite differences\n",
      "[[ -2.243]\n",
      " [  2.474]\n",
      " [  3.406]\n",
      " [ -1.942]\n",
      " [-12.468]\n",
      " [  0.   ]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Bias 1, derivatives from backprop:\n",
      "[[-0.   ]\n",
      " [-5.648]\n",
      " [ 0.   ]\n",
      " [ 0.   ]\n",
      " [-5.361]\n",
      " [ 0.   ]]\n",
      "Bias 1, derivatives from finite differences\n",
      "[[ 0.   ]\n",
      " [-5.648]\n",
      " [ 0.   ]\n",
      " [ 0.   ]\n",
      " [-5.361]\n",
      " [ 0.   ]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Bias 2, derivatives from backprop:\n",
      "[[-0.   ]\n",
      " [-0.   ]\n",
      " [ 0.469]\n",
      " [ 0.   ]\n",
      " [-4.997]\n",
      " [ 0.254]]\n",
      "Bias 2, derivatives from finite differences\n",
      "[[ 0.   ]\n",
      " [ 0.   ]\n",
      " [ 0.469]\n",
      " [ 0.   ]\n",
      " [-4.997]\n",
      " [ 0.254]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Bias 3, derivatives from backprop:\n",
      "[[-0.   ]\n",
      " [-2.4  ]\n",
      " [-0.831]\n",
      " [-0.   ]\n",
      " [ 1.697]\n",
      " [ 2.695]]\n",
      "Bias 3, derivatives from finite differences\n",
      "[[ 0.   ]\n",
      " [-2.4  ]\n",
      " [-0.831]\n",
      " [ 0.   ]\n",
      " [ 1.697]\n",
      " [ 2.695]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Bias 4, derivatives from backprop:\n",
      "[[-0.   ]\n",
      " [ 0.   ]\n",
      " [ 0.   ]\n",
      " [-0.   ]\n",
      " [-2.606]\n",
      " [-0.   ]]\n",
      "Bias 4, derivatives from finite differences\n",
      "[[ 0.   ]\n",
      " [ 0.   ]\n",
      " [ 0.   ]\n",
      " [ 0.   ]\n",
      " [-2.606]\n",
      " [ 0.   ]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Bias 5, derivatives from backprop:\n",
      "[[-18.093]]\n",
      "Bias 5, derivatives from finite differences\n",
      "[[-18.093]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Weight 0, derivatives from backprop:\n",
      "[[ -2.692]\n",
      " [  2.968]\n",
      " [  4.087]\n",
      " [ -2.33 ]\n",
      " [-14.961]\n",
      " [  0.   ]]\n",
      "Weight 0, derivatives from finite differences\n",
      "[[ -2.692]\n",
      " [  2.968]\n",
      " [  4.087]\n",
      " [ -2.33 ]\n",
      " [-14.961]\n",
      " [  0.   ]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Weight 1, derivatives from backprop:\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [-16.256  -3.4    -9.141 -17.074 -21.098   0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [-15.428  -3.227  -8.676 -16.205 -20.024   0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]]\n",
      "Weight 1, derivatives from finite differences\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [-16.256  -3.4    -9.141 -17.074 -21.098   0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [-15.428  -3.227  -8.676 -16.205 -20.024   0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Weight 2, derivatives from backprop:\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      2.686   0.      0.      1.572   0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.    -28.616   0.      0.    -16.753   0.   ]\n",
      " [  0.      1.453   0.      0.      0.851   0.   ]]\n",
      "Weight 2, derivatives from finite differences\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      2.686   0.      0.      1.572   0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.    -28.616   0.      0.    -16.753   0.   ]\n",
      " [  0.      1.453   0.      0.      0.851   0.   ]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Weight 3, derivatives from backprop:\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.     -1.837   0.    -21.453  -5.499]\n",
      " [  0.      0.     -0.636   0.     -7.425  -1.903]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      1.299   0.     15.166   3.888]\n",
      " [  0.      0.      2.063   0.     24.094   6.176]]\n",
      "Weight 3, derivatives from finite differences\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.     -1.837   0.    -21.453  -5.499]\n",
      " [  0.      0.     -0.636   0.     -7.425  -1.903]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      1.299   0.     15.166   3.888]\n",
      " [  0.      0.      2.063   0.     24.094   6.176]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Weight 4, derivatives from backprop:\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.    -40.818 -24.568   0.    -11.003  -5.073]\n",
      " [  0.      0.      0.      0.      0.      0.   ]]\n",
      "Weight 4, derivatives from finite differences\n",
      "[[  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.      0.      0.      0.      0.      0.   ]\n",
      " [  0.    -40.818 -24.568   0.    -11.003  -5.073]\n",
      " [  0.      0.      0.      0.      0.      0.   ]]\n",
      "Success!  Derivatives match.\n",
      "-----------------------------------------------\n",
      "Weight 5, derivatives from backprop:\n",
      "[[   0.       0.       0.       0.    -200.165    0.   ]]\n",
      "Weight 5, derivatives from finite differences\n",
      "[[   0.       0.       0.       0.    -200.165    0.   ]]\n",
      "Success!  Derivatives match.\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "# Make space for derivatives computed by finite differences\n",
    "all_dl_dweights_fd = [None] * (K+1)\n",
    "all_dl_dbiases_fd = [None] * (K+1)\n",
    "\n",
    "# Let's test if we have the derivatives right using finite differences\n",
    "delta_fd = 0.000001\n",
    "\n",
    "# Test the dervatives of the bias vectors\n",
    "for layer in range(K+1):\n",
    "  dl_dbias  = np.zeros_like(all_dl_dbiases[layer])\n",
    "  # For every element in the bias\n",
    "  for row in range(all_biases[layer].shape[0]):\n",
    "    # Take copy of biases  We'll change one element each time\n",
    "    all_biases_copy = [np.array(x) for x in all_biases]\n",
    "    all_biases_copy[layer][row] += delta_fd\n",
    "    network_output_1, *_ = compute_network_output(net_input, all_weights, all_biases_copy)\n",
    "    network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
    "    dl_dbias[row] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
    "  all_dl_dbiases_fd[layer] = np.array(dl_dbias)\n",
    "  print(\"-----------------------------------------------\")\n",
    "  print(\"Bias %d, derivatives from backprop:\"%(layer))\n",
    "  print(all_dl_dbiases[layer])\n",
    "  print(\"Bias %d, derivatives from finite differences\"%(layer))\n",
    "  print(all_dl_dbiases_fd[layer])\n",
    "  if np.allclose(all_dl_dbiases_fd[layer],all_dl_dbiases[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
    "    print(\"Success!  Derivatives match.\")\n",
    "  else:\n",
    "    print(\"Failure!  Derivatives different.\")\n",
    "\n",
    "# Test the derivatives of the weights matrices\n",
    "for layer in range(K+1):\n",
    "  dl_dweight  = np.zeros_like(all_dl_dweights[layer])\n",
    "  # For every element in the bias\n",
    "  for row in range(all_weights[layer].shape[0]):\n",
    "    for col in range(all_weights[layer].shape[1]):\n",
    "      # Take copy of biases  We'll change one element each time\n",
    "      all_weights_copy = [np.array(x) for x in all_weights]\n",
    "      all_weights_copy[layer][row][col] += delta_fd\n",
    "      network_output_1, *_ = compute_network_output(net_input, all_weights_copy, all_biases)\n",
    "      network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
    "      dl_dweight[row][col] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
    "  all_dl_dweights_fd[layer] = np.array(dl_dweight)\n",
    "  print(\"-----------------------------------------------\")\n",
    "  print(\"Weight %d, derivatives from backprop:\"%(layer))\n",
    "  print(all_dl_dweights[layer])\n",
    "  print(\"Weight %d, derivatives from finite differences\"%(layer))\n",
    "  print(all_dl_dweights_fd[layer])\n",
    "  if np.allclose(all_dl_dweights_fd[layer],all_dl_dweights[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
    "    print(\"Success!  Derivatives match.\")\n",
    "  else:\n",
    "    print(\"Failure!  Derivatives different.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
