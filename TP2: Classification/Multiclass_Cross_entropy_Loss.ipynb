{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSlFkICHwHQF"
   },
   "source": [
    "# **Multiclass Cross-Entropy Loss**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PYMZ1x-Pv1ht"
   },
   "outputs": [],
   "source": [
    "# Imports math library\n",
    "import numpy as np\n",
    "# Used for repmat\n",
    "import numpy.matlib\n",
    "# Imports plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "# Import math Library\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rectified Linear Unit (ReLU) function\n",
    "def ReLU(preactivation):\n",
    "    return np.maximum(0, preactivation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Fv7SZR3tv7mV"
   },
   "outputs": [],
   "source": [
    "# Define a shallow neural network\n",
    "def shallow_nn(x, beta_0, omega_0, beta_1, omega_1):\n",
    "    z0 = np.zeros((x.shape[0], beta_0.shape[0]))\n",
    "    \n",
    "    # Calculate pre-activation for each neuron\n",
    "    for i in range(x.shape[0]):\n",
    "        # beta_0 is the bias term, omega_0 * x[i] is the weighted input\n",
    "        z0[i, :] = beta_0.flatten() + omega_0.flatten() * x[i, 0]\n",
    "    \n",
    "    # Apply ReLU activation\n",
    "    a0 = ReLU(z0)\n",
    "    # Calculate output layer - omega_1 is 1x3, a0 is nx3\n",
    "    # We need to calculate for each input row\n",
    "    z1 = np.zeros((x.shape[0], 1))\n",
    "    for i in range(x.shape[0]):\n",
    "        z1[i, 0] = beta_1[0, 0] + np.dot(omega_1, a0[i, :].reshape(-1, 1))[0, 0]\n",
    "    \n",
    "    # Calculate probability using sigmoid function\n",
    "    lambda_x = 1 / (1 + np.exp(-z1))\n",
    "    \n",
    "    return z1, lambda_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a shallow neural network\n",
    "def shallow_nn(x, beta_0, omega_0, beta_1, omega_1):\n",
    "    # First, reshape x into a column vector if it's not already\n",
    "    x = np.atleast_2d(x).reshape(-1, 1)\n",
    "    \n",
    "    # Calculate the hidden layer pre-activations\n",
    "    # For each input x, add the bias beta_0 and multiply by weights omega_0\n",
    "    z = beta_0 + omega_0 * x\n",
    "    \n",
    "    # Apply ReLU activation function to get hidden layer activations\n",
    "    a = ReLU(z)\n",
    "    \n",
    "    # Calculate the output layer\n",
    "    # a has shape (3, n_samples), omega_1 has shape (3, 3)\n",
    "    # We want to calculate beta_1 + omega_1 @ a for each sample\n",
    "    # First, transpose omega_1 to make matrix multiplication work correctly\n",
    "    out = beta_1 + omega_1 @ a\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    exp_out = np.exp(out)\n",
    "    lambda_out = exp_out / np.sum(exp_out, axis=0)\n",
    "    \n",
    "    return out, lambda_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pUT9Ain_HRim"
   },
   "outputs": [],
   "source": [
    "# Get parameters for model -- we can call this function to easily reset them\n",
    "def get_parameters():\n",
    "  # And we'll create a network that approximately fits it\n",
    "  beta_0 = np.zeros((3,1));  # formerly theta_x0\n",
    "  omega_0 = np.zeros((3,1)); # formerly theta_x1\n",
    "  beta_1 = np.zeros((3,1));  # NOTE -- there are three outputs now (one for each class, so three output biases)\n",
    "  omega_1 = np.zeros((3,3)); # NOTE -- there are three outputs now (one for each class, so nine output weights, connecting 3 hidden units to 3 outputs)\n",
    "\n",
    "  beta_0[0,0] = 0.3; beta_0[1,0] = -1.0; beta_0[2,0] = -0.5\n",
    "  omega_0[0,0] = -1.0; omega_0[1,0] = 1.8; omega_0[2,0] = 0.65\n",
    "  beta_1[0,0] = 2.0; beta_1[1,0] = -2; beta_1[2,0] = 0.0\n",
    "  omega_1[0,0] = -24.0; omega_1[0,1] = -8.0; omega_1[0,2] = 50.0\n",
    "  omega_1[1,0] = -2.0; omega_1[1,1] = 8.0; omega_1[1,2] = -30.0\n",
    "  omega_1[2,0] = 16.0; omega_1[2,1] = -8.0; omega_1[2,2] =-8\n",
    "\n",
    "  return beta_0, omega_0, beta_1, omega_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NRR67ri_1TzN"
   },
   "outputs": [],
   "source": [
    "# Utility function for plotting data\n",
    "def plot_multiclass_classification(x_model, out_model, lambda_model, x_data = None, y_data = None, title= None):\n",
    "  # Make sure model data are 1D arrays\n",
    "  n_data = len(x_model)\n",
    "  n_class = 3\n",
    "  x_model = np.squeeze(x_model)\n",
    "  out_model = np.reshape(out_model, (n_class,n_data))\n",
    "  lambda_model = np.reshape(lambda_model, (n_class,n_data))\n",
    "\n",
    "  fig, ax = plt.subplots(1,2)\n",
    "  fig.set_size_inches(7.0, 3.5)\n",
    "  fig.tight_layout(pad=3.0)\n",
    "  ax[0].plot(x_model,out_model[0,:],'r-')\n",
    "  ax[0].plot(x_model,out_model[1,:],'g-')\n",
    "  ax[0].plot(x_model,out_model[2,:],'b-')\n",
    "  ax[0].set_xlabel('Input, $x$'); ax[0].set_ylabel('Model outputs')\n",
    "  ax[0].set_xlim([0,1]);ax[0].set_ylim([-4,4])\n",
    "  if title is not None:\n",
    "    ax[0].set_title(title)\n",
    "  ax[1].plot(x_model,lambda_model[0,:],'r-')\n",
    "  ax[1].plot(x_model,lambda_model[1,:],'g-')\n",
    "  ax[1].plot(x_model,lambda_model[2,:],'b-')\n",
    "  ax[1].set_xlabel('Input, $x$'); ax[1].set_ylabel('$\\lambda$ or Pr(y=k|x)')\n",
    "  ax[1].set_xlim([0,1]);ax[1].set_ylim([-0.1,1.05])\n",
    "  if title is not None:\n",
    "    ax[1].set_title(title)\n",
    "  if x_data is not None:\n",
    "    for i in range(len(x_data)):\n",
    "      if y_data[i] ==0:\n",
    "        ax[1].plot(x_data[i],-0.05, 'r.')\n",
    "      if y_data[i] ==1:\n",
    "        ax[1].plot(x_data[i],-0.05, 'g.')\n",
    "      if y_data[i] ==2:\n",
    "        ax[1].plot(x_data[i],-0.05, 'b.')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 100 into shape (3,100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m out_model, lambda_model \u001b[38;5;241m=\u001b[39m shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m plot_multiclass_classification(x_model, out_model, lambda_model, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShallow Neural Network\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mplot_multiclass_classification\u001b[0;34m(x_model, out_model, lambda_model, x_data, y_data, title)\u001b[0m\n\u001b[1;32m      5\u001b[0m n_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      6\u001b[0m x_model \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(x_model)\n\u001b[0;32m----> 7\u001b[0m out_model \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(out_model, (n_class,n_data))\n\u001b[1;32m      8\u001b[0m lambda_model \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(lambda_model, (n_class,n_data))\n\u001b[1;32m     10\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:285\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m'\u001b[39m, newshape, order\u001b[38;5;241m=\u001b[39morder)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 100 into shape (3,100)"
     ]
    }
   ],
   "source": [
    "# Get the parameters\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "\n",
    "# Create input data points\n",
    "x_model = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "\n",
    "# Run the model\n",
    "out_model, lambda_model = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "\n",
    "# Plot the results\n",
    "plot_multiclass_classification(x_model, out_model, lambda_model, title=\"Shallow Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsgLZwsPxauP",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Multiclass classification\n",
    "\n",
    "For multiclass classification, the network must predict the probability of $K$ classes, using $K$ outputs.  However, these probability must be non-negative and sum to one, and the network outputs can take arbitrary values.  Hence, we pass the outputs through a softmax function which maps $K$ arbitrary values to $K$ non-negative values that sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFb8h-9IXnIe"
   },
   "outputs": [],
   "source": [
    "# Softmax function that maps a vector of arbitrary values to a vector of values that are positive and sum to one.\n",
    "def softmax(model_out):\n",
    "    # TODO -- implement the softmax function   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "id": "VWzNOt1swFVd",
    "outputId": "125ccd43-f1bb-4c25-8310-60962f25b614"
   },
   "outputs": [],
   "source": [
    "# Let's create some 1D training data\n",
    "x_train = np.array([0.09291784,0.46809093,0.93089486,0.67612654,0.73441752,0.86847339,\\\n",
    "                   0.49873225,0.51083168,0.18343972,0.99380898,0.27840809,0.38028817,\\\n",
    "                   0.12055708,0.56715537,0.92005746,0.77072270,0.85278176,0.05315950,\\\n",
    "                   0.87168699,0.58858043])\n",
    "y_train = np.array([2,0,1,2,1,0,\\\n",
    "                    0,2,2,0,2,0,\\\n",
    "                    2,0,1,2,1,2, \\\n",
    "                    1,0])\n",
    "\n",
    "# Get parameters for the model\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "\n",
    "# Define a range of input values\n",
    "x_model = np.arange(0,1,0.01)\n",
    "# Run the model to get values to plot and plot it.\n",
    "model_out= shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "lambda_model = softmax(model_out)\n",
    "plot_multiclass_classification(x_model, model_out, lambda_model, x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaLdRlEX0FkU"
   },
   "outputs": [],
   "source": [
    "# Return probability under categorical distribution for observed class y\n",
    "# Just take value from row k of lambda param where y =k,\n",
    "def categorical_distribution(y, lambda_param):\n",
    "  # TODO-- write in the equation for the categorical distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4TSL14dqHHbV",
    "outputId": "e0dd4b73-e195-48e5-e125-2110296085ba"
   },
   "outputs": [],
   "source": [
    "# Let's double check we get the right answer before proceeding\n",
    "print(\"Correct answer = %3.3f, Your answer = %3.3f\"%(0.2,categorical_distribution(np.array([[0]]),np.array([[0.2],[0.5],[0.3]]))))\n",
    "print(\"Correct answer = %3.3f, Your answer = %3.3f\"%(0.5,categorical_distribution(np.array([[1]]),np.array([[0.2],[0.5],[0.3]]))))\n",
    "print(\"Correct answer = %3.3f, Your answer = %3.3f\"%(0.3,categorical_distribution(np.array([[2]]),np.array([[0.2],[0.5],[0.3]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5z_0dzQMF35"
   },
   "source": [
    "Now let's compute the likelihood using this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpS7o6liCx7f"
   },
   "outputs": [],
   "source": [
    "# Return the likelihood of all of the data under the model\n",
    "def compute_likelihood(y_train, lambda_param):\n",
    "  # TODO-- compute the likelihood of the data -- the product of the Bernoulli probabilities for each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hQxBLoVNlr2",
    "outputId": "cff2c89e-03d6-49f7-d7ca-a21fd730d966"
   },
   "outputs": [],
   "source": [
    "# Let's test this\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "# Use our neural network to predict the parameters of the categorical distribution\n",
    "model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "lambda_train = softmax(model_out)\n",
    "#print(\"lambda train=\",lambda_train)\n",
    "# Compute the likelihood\n",
    "likelihood = compute_likelihood(y_train, lambda_train)\n",
    "# Let's double check we get the right answer before proceeding\n",
    "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(0.000000041,likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzphKgPfOvlk"
   },
   "source": [
    "You can see that this gives a very small answer, even for this small 1D dataset, and with the model fitting quite well.  This is because it is the product of several probabilities, which are all quite small themselves.\n",
    "This will get out of hand pretty quickly with real datasets -- the likelihood will get so small that we can't represent it with normal finite-precision math\n",
    "\n",
    "This is why we use negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsT0CWiKBmTV"
   },
   "outputs": [],
   "source": [
    "# Return the negative log likelihood of the data under the model\n",
    "def compute_negative_log_likelihood(y_train, lambda_param):\n",
    "    # TODO -- compute the likelihood of the data -- don't use the likelihood function above -- compute the negative sum of the log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nVxUXg9rQmwI",
    "outputId": "7ff40103-2bf8-4fed-98f9-d1f9ea32e188"
   },
   "outputs": [],
   "source": [
    "# Let's test this\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "# Use our neural network to predict the parameters of the categorical distribution\n",
    "model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "# Pass the outputs through the softmax function\n",
    "lambda_train = softmax(model_out)\n",
    "# Compute the negative log likelihood\n",
    "nll = compute_negative_log_likelihood(y_train, lambda_train)\n",
    "# Let's double check we get the right answer before proceeding\n",
    "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(17.015457867,nll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgcRojvPWh4V"
   },
   "source": [
    "Now let's investigate finding the maximum likelihood / minimum negative log likelihood solution.  For simplicity, we'll assume that all the parameters are fixed except one and look at how the likelihood and negative log likelihood change as we manipulate the last parameter.  We'll start with overall y_offset, $\\beta_1$ (formerly $\\phi_0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pFKtDaAeVU4U",
    "outputId": "f809a5e0-5df4-48a9-b60e-5a85a1908be9"
   },
   "outputs": [],
   "source": [
    "# Define a range of values for the parameter\n",
    "beta_1_vals = np.arange(-2,6.0,0.1)\n",
    "# Create some arrays to store the likelihoods, negative log likelihoods\n",
    "likelihoods = np.zeros_like(beta_1_vals)\n",
    "nlls = np.zeros_like(beta_1_vals)\n",
    "\n",
    "# Initialise the parameters\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "for count in range(len(beta_1_vals)):\n",
    "  # Set the value for the parameter\n",
    "  beta_1[0,0] = beta_1_vals[count]\n",
    "  # Run the network with new parameters\n",
    "  model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "  lambda_train = softmax(model_out)\n",
    "  # Compute and store the two values\n",
    "  likelihoods[count] = compute_likelihood(y_train,lambda_train)\n",
    "  nlls[count] = compute_negative_log_likelihood(y_train, lambda_train)\n",
    "  # Draw the model for every 20th parameter setting\n",
    "  if count % 20 == 0:\n",
    "    # Run the model to get values to plot and plot it.\n",
    "    model_out = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "    lambda_model = softmax(model_out)\n",
    "    plot_multiclass_classification(x_model, model_out, lambda_model, x_train, y_train, title=\"beta1[0,0]=%3.3f\"%(beta_1[0,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "UHXeTa9MagO6",
    "outputId": "4b0c132c-bb3e-4401-c808-38897b950ff5"
   },
   "outputs": [],
   "source": [
    "# Now let's plot the likelihood and negative log likelihood as a function of the value of the offset beta1\n",
    "fig, ax = plt.subplots()\n",
    "fig.tight_layout(pad=5.0)\n",
    "likelihood_color = 'tab:red'\n",
    "nll_color = 'tab:blue'\n",
    "\n",
    "ax.set_xlabel('beta_1[0, 0]')\n",
    "ax.set_ylabel('likelihood', color = likelihood_color)\n",
    "ax.plot(beta_1_vals, likelihoods, color = likelihood_color)\n",
    "ax.tick_params(axis='y', labelcolor=likelihood_color)\n",
    "\n",
    "ax1 = ax.twinx()\n",
    "ax1.plot(beta_1_vals, nlls, color = nll_color)\n",
    "ax1.set_ylabel('negative log likelihood', color = nll_color)\n",
    "ax1.tick_params(axis='y', labelcolor = nll_color)\n",
    "\n",
    "plt.axvline(x = beta_1_vals[np.argmax(likelihoods)], linestyle='dotted')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "aDEPhddNdN4u",
    "outputId": "21f7101f-033e-4c26-863b-c8781d5f8ccf"
   },
   "outputs": [],
   "source": [
    "# Hopefully, you can see that the maximum of the likelihood fn is at the same position as the minimum negative log likelihood solution\n",
    "# Let's check that:\n",
    "print(\"Maximum likelihood = %9.9f, at beta_1=%3.3f\"%( (likelihoods[np.argmax(likelihoods)],beta_1_vals[np.argmax(likelihoods)])))\n",
    "print(\"Minimum negative log likelihood = %f, at beta_1=%3.3f\"%( (nlls[np.argmin(nlls)],beta_1_vals[np.argmin(nlls)])))\n",
    "\n",
    "# Plot the best model\n",
    "beta_1[0,0] = beta_1_vals[np.argmin(nlls)]\n",
    "model_out = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "lambda_model = softmax(model_out)\n",
    "plot_multiclass_classification(x_model, model_out, lambda_model, x_train, y_train, title=\"beta1[0,0]=%3.3f\"%(beta_1[0,0]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
