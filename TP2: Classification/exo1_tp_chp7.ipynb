{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSlFkICHwHQF"
   },
   "source": [
    "# **Binary Cross-Entropy Loss**\n",
    "\n",
    "Binary cross-entropy loss based on the Bernoulli distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PYMZ1x-Pv1ht"
   },
   "outputs": [],
   "source": [
    "# Imports math library\n",
    "import numpy as np\n",
    "# Imports plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "# Import math Library\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rectified Linear Unit (ReLU) function\n",
    "def ReLU(preactivation):\n",
    "    # TODO -- implement the ReLU function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fv7SZR3tv7mV"
   },
   "outputs": [],
   "source": [
    "# Define a shallow neural network\n",
    "def shallow_nn(x, beta_0, omega_0, beta_1, omega_1): \n",
    "    # TODO -- implement the shallow NN using matrix notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pUT9Ain_HRim"
   },
   "outputs": [],
   "source": [
    "# Get parameters for model -- we can call this function to easily reset them\n",
    "def get_parameters():\n",
    "  # And we'll create a network that approximately fits it\n",
    "  beta_0 = np.zeros((3,1));  # formerly theta_x0\n",
    "  omega_0 = np.zeros((3,1)); # formerly theta_x1\n",
    "  beta_1 = np.zeros((1,1));  # formerly phi_0\n",
    "  omega_1 = np.zeros((1,3)); # formerly phi_x\n",
    "\n",
    "  beta_0[0,0] = 0.3; beta_0[1,0] = -1.0; beta_0[2,0] = -0.5\n",
    "  omega_0[0,0] = -1.0; omega_0[1,0] = 1.8; omega_0[2,0] = 0.65\n",
    "  beta_1[0,0] = 2.6\n",
    "  omega_1[0,0] = -24.0; omega_1[0,1] = -8.0; omega_1[0,2] = 50.0\n",
    "\n",
    "  return beta_0, omega_0, beta_1, omega_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NRR67ri_1TzN"
   },
   "outputs": [],
   "source": [
    "# Utility function for plotting data\n",
    "def plot_binary_classification(x_model, out_model, lambda_model, x_data = None, y_data = None, title= None):\n",
    "  # Make sure model data are 1D arrays\n",
    "  x_model = np.squeeze(x_model)\n",
    "  out_model = np.squeeze(out_model)\n",
    "  lambda_model = np.squeeze(lambda_model)\n",
    "\n",
    "  fig, ax = plt.subplots(1,2)\n",
    "  fig.set_size_inches(7.0, 3.5)\n",
    "  fig.tight_layout(pad=3.0)\n",
    "  ax[0].plot(x_model,out_model)\n",
    "  ax[0].set_xlabel('Input, $x$'); ax[0].set_ylabel('Model output')\n",
    "  ax[0].set_xlim([0,1]);ax[0].set_ylim([-4,4])\n",
    "  if title is not None:\n",
    "    ax[0].set_title(title)\n",
    "  ax[1].plot(x_model,lambda_model)\n",
    "  ax[1].set_xlabel('Input, $x$'); ax[1].set_ylabel('$\\lambda$ or Pr(y=1|x)')\n",
    "  ax[1].set_xlim([0,1]);ax[1].set_ylim([-0.05,1.05])\n",
    "  if title is not None:\n",
    "    ax[1].set_title(title)\n",
    "  if x_data is not None:\n",
    "    ax[1].plot(x_data, y_data, 'ko')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsgLZwsPxauP"
   },
   "source": [
    "# Binary classification\n",
    "\n",
    "In binary classification tasks, the network predicts the probability of the output belonging to class 1.  Since probabilities must lie in [0,1] and the network can output arbitrary values, we map the network through a sigmoid function that ensures the range is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFb8h-9IXnIe"
   },
   "outputs": [],
   "source": [
    "def sigmoid(model_out):\n",
    "  # TODO -- implement the logistic sigmoid function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "VWzNOt1swFVd",
    "outputId": "53b01e7d-5402-4992-a273-6f585e7b2dd2"
   },
   "outputs": [],
   "source": [
    "# Let's create some 1D training data\n",
    "x_train = np.array([0.09291784,0.46809093,0.93089486,0.67612654,0.73441752,0.86847339,\\\n",
    "                   0.49873225,0.51083168,0.18343972,0.99380898,0.27840809,0.38028817,\\\n",
    "                   0.12055708,0.56715537,0.92005746,0.77072270,0.85278176,0.05315950,\\\n",
    "                   0.87168699,0.58858043])\n",
    "y_train = np.array([0,1,1,0,0,1,\\\n",
    "                    1,0,0,1,0,1,\\\n",
    "                    0,1,1,0,1,0, \\\n",
    "                    1,1])\n",
    "\n",
    "# Get parameters for the model\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "\n",
    "# Define a range of input values\n",
    "x_model = np.arange(0, 1, 0.01)\n",
    "# Run the model to get values to plot and plot it.\n",
    "model_out= shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "#print(model_out)\n",
    "lambda_model = sigmoid(model_out)\n",
    "#print(lambda_model)\n",
    "plot_binary_classification(x_model, model_out, lambda_model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_815ozau7mKO"
   },
   "outputs": [],
   "source": [
    "def bernoulli_distribution(y, lambda_param):\n",
    "    # TODO-- write in the equation for the Bernoulli distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPEyBD6k7gYx",
    "outputId": "42839445-5f6c-4e73-9b2e-fdc58d460143"
   },
   "outputs": [],
   "source": [
    "# Let's double check we get the right answer before proceeding\n",
    "print(\"Correct answer = %3.3f, Your answer = %3.3f\"%(0.8, bernoulli_distribution(0, 0.2)))\n",
    "print(\"Correct answer = %3.3f, Your answer = %3.3f\"%(0.2, bernoulli_distribution(1.0, 0.2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5z_0dzQMF35"
   },
   "source": [
    "Now let's compute the likelihood using this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpS7o6liCx7f"
   },
   "outputs": [],
   "source": [
    "# Return the likelihood of all of the data under the model\n",
    "  # TODO-- compute def compute_likelihood(y_train, lambda_param):\n",
    "the likelihood of the data -- the product of the Bernoulli probabilities for each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hQxBLoVNlr2",
    "outputId": "8d72c700-f728-4392-b9d5-7cf0efb08a67"
   },
   "outputs": [],
   "source": [
    "# Let's test this\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "# Use our neural network to predict the Bernoulli parameter lambda\n",
    "model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "lambda_train = sigmoid(model_out)\n",
    "# Compute the likelihood\n",
    "likelihood = compute_likelihood(y_train, lambda_train)\n",
    "# Let's double check we get the right answer before proceeding\n",
    "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(0.000070237,likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzphKgPfOvlk"
   },
   "source": [
    "You can see that this gives a very small answer, even for this small 1D dataset, and with the model fitting quite well.  This is because it is the product of several probabilities, which are all quite small themselves.\n",
    "This will get out of hand pretty quickly with real datasets -- the likelihood will get so small that we can't represent it with normal finite-precision math\n",
    "\n",
    "This is why we use negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsT0CWiKBmTV"
   },
   "outputs": [],
   "source": [
    "# Return the negative log likelihood of the data under the model\n",
    "def compute_negative_log_likelihood(y_train, lambda_param):\n",
    "  # TODO -- compute the likelihood of the data -- don't use the likelihood function above -- compute the negative sum of the log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nVxUXg9rQmwI",
    "outputId": "957ca19d-c3de-438b-8170-8dceb6f88d3d"
   },
   "outputs": [],
   "source": [
    "# Let's test this\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "# Use our neural network to predict the mean of the Gaussian\n",
    "model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "# Pass through the sigmoid function\n",
    "lambda_train = sigmoid(model_out)\n",
    "# Compute the log likelihood\n",
    "nll = compute_negative_log_likelihood(y_train, lambda_train)\n",
    "# Let's double check we get the right answer before proceeding\n",
    "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(9.563639387,nll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgcRojvPWh4V"
   },
   "source": [
    "Now let's investigate finding the maximum likelihood / minimum negative log likelihood solution.  For simplicity, we'll assume that all the parameters are fixed except one and look at how the likelihood and negative log likelihood change as we manipulate the last parameter.  We'll start with overall y_offset, beta_1 (formerly phi_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pFKtDaAeVU4U",
    "outputId": "0f9c5fb2-c2a3-41d8-9a14-87808c62d022"
   },
   "outputs": [],
   "source": [
    "# Define a range of values for the parameter\n",
    "beta_1_vals = np.arange(-2, 6.0, 0.1)\n",
    "# Create some arrays to store the likelihoods, negative log likelihoods\n",
    "likelihoods = np.zeros_like(beta_1_vals)\n",
    "nlls = np.zeros_like(beta_1_vals)\n",
    "# Initialise the parameters\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "for count in range(len(beta_1_vals)):\n",
    "  # Set the value for the parameter\n",
    "  beta_1[0,0] = beta_1_vals[count]\n",
    "  # Run the network with new parameters\n",
    "  model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "  lambda_train = sigmoid(model_out)\n",
    "  # Compute and store the two values\n",
    "  likelihoods[count] = compute_likelihood(y_train,lambda_train)\n",
    "  nlls[count] = compute_negative_log_likelihood(y_train, lambda_train)\n",
    "  # Draw the model for every 20th parameter setting\n",
    "  if count % 20 == 0:\n",
    "    # Run the model to get values to plot and plot it.\n",
    "    model_out = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "    lambda_model = sigmoid(model_out)\n",
    "    plot_binary_classification(x_model, model_out, lambda_model, x_train, y_train, title=\"beta_1[0]=%3.3f\"%(beta_1[0,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "UHXeTa9MagO6",
    "outputId": "f6b86839-1ccd-48a6-a14f-e7ab7be67937"
   },
   "outputs": [],
   "source": [
    "# Now let's plot the likelihood and negative log likelihood as a function of the value of the offset beta1\n",
    "fig, ax = plt.subplots()\n",
    "fig.tight_layout(pad=5.0)\n",
    "likelihood_color = 'tab:red'\n",
    "nll_color = 'tab:blue'\n",
    "\n",
    "ax.set_xlabel('beta_1[0]')\n",
    "ax.set_ylabel('likelihood', color = likelihood_color)\n",
    "ax.plot(beta_1_vals, likelihoods, color = likelihood_color)\n",
    "ax.tick_params(axis='y', labelcolor=likelihood_color)\n",
    "\n",
    "ax1 = ax.twinx()\n",
    "ax1.plot(beta_1_vals, nlls, color = nll_color)\n",
    "ax1.set_ylabel('negative log likelihood', color = nll_color)\n",
    "ax1.tick_params(axis='y', labelcolor = nll_color)\n",
    "\n",
    "plt.axvline(x = beta_1_vals[np.argmax(likelihoods)], linestyle='dotted')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "aDEPhddNdN4u",
    "outputId": "001eaf23-7417-4a9d-959a-3a8d23ec4b26"
   },
   "outputs": [],
   "source": [
    "# Hopefully, you can see that the maximum of the likelihood fn is at the same position as the minimum negative log likelihood\n",
    "# Let's check that:\n",
    "print(\"Maximum likelihood = %f, at beta_1=%3.3f\"%((likelihoods[np.argmax(likelihoods)],beta_1_vals[np.argmax(likelihoods)])))\n",
    "print(\"Minimum negative log likelihood = %f, at beta_1=%3.3f\"%( (nlls[np.argmin(nlls)],beta_1_vals[np.argmin(nlls)])))\n",
    "\n",
    "# Plot the best model\n",
    "beta_1[0,0] = beta_1_vals[np.argmin(nlls)]\n",
    "model_out = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "lambda_model = sigmoid(model_out)\n",
    "plot_binary_classification(x_model, model_out, lambda_model, x_train, y_train, title=\"beta_1[0]=%3.3f\"%(beta_1[0,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of values for the parameter\n",
    "omega_0_0_vals = np.arange(-2.0, 0.0, 0.1)\n",
    "print (omega_0_0_vals.shape)\n",
    "# Create some arrays to store the likelihoods, negative log likelihoods\n",
    "likelihoods = np.zeros_like(omega_0_0_vals)\n",
    "nlls = np.zeros_like(omega_0_0_vals)\n",
    "\n",
    "# Initialise the parameters\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "for count in range(len(omega_0_0_vals)):\n",
    "  # Set the value for the parameter\n",
    "  omega_0[0,0] = omega_0_0_vals[count]\n",
    "  # Run the network with new parameters\n",
    "  model_out = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "  lambda_train = sigmoid(model_out)\n",
    "  # Compute and store the two values\n",
    "  likelihoods[count] = compute_likelihood(y_train,lambda_train)\n",
    "  nlls[count] = compute_negative_log_likelihood(y_train, lambda_train)\n",
    "  # Draw the model for every 20th parameter setting\n",
    "  if count % 5 == 0:\n",
    "    # Run the model to get values to plot and plot it.\n",
    "    model_out = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "    lambda_model = sigmoid(model_out)\n",
    "    plot_binary_classification(x_model, model_out, lambda_model, x_train, y_train, title=\"omega_0[0]=%3.3f\"%(omega_0[0,0]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
